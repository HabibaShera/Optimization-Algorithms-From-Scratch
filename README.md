# Optimization Techniques from Scratch

This repository contains implementations of various optimization techniques from scratch in Python. These techniques are fundamental in the field of machine learning and optimization and are often used to optimize the parameters of models during training. Each technique is implemented in a separate Python file.

## Implemented Techniques

Sure, here are the numbers from the list:

1. AdaGrad Based GD Single Variable.py
2. Adam Based Batch Gradient Descent Multivariable.py
3. Adam Based GD Single Variable.py
4. Adam Based Mini-Batch GD Multivariable.py
7. Gradient Descent Multi-Variables.py
8. Gradient Descent Single Variable.py
9. Mini-Batch GD Single Variable.py
10. Momentum Based GD Single Variable.py
11. NAG Based GD Single Variable.py
12. NAG Based Stochastic GD Multivariable.py
13. Newton method Multivariable.py
14. Newton Method.py
16. RMSProp Based GD Single Variable.py
17. Stochastic Gradient Descent Single Variable.py

## How to Use

Each Python file contains the implementation of a specific optimization technique. To use any of these techniques, follow these steps:

1. Clone this repository to your local machine:
   ```
   git clone https://github.com/HabibaShera/Optimization-Algorithms-From-Scratch.git
   ```

2. Navigate to the repository directory:
   ```
   cd Optimization-Algorithms-From-Scratch
   ```

3. Choose the optimization technique you want to use and run the corresponding Python file. You may need to modify the code to suit your specific use case.

4. Optionally, explore the notebooks and algorithm analysis provided in the `Notebooks & Algorithm Analysis` folder for further understanding and insights into these optimization techniques.

## Contributions

Contributions to this repository are welcome. If you'd like to add more optimization techniques, improve existing implementations, or fix any issues, feel free to open a pull request.
